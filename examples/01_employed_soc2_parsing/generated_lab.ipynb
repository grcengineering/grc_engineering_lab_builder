{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12-Week Personalized Learning Lab: SOC 2 Document Analysis Automation\n",
    "\n",
    "**Generated for:** TPRM Lead, Midsize B2B SaaS Company\n",
    "\n",
    "**Goal:** Reduce SOC 2 review time from 3+ hours to 30-45 minutes through programmatic parsing\n",
    "\n",
    "**Time Commitment:** 3 hours per week\n",
    "\n",
    "**Your Context:**\n",
    "- 500+ vendors requiring annual reviews\n",
    "- Current manual process using Google Workspace\n",
    "- Intermediate Python skills\n",
    "- Board requesting metrics dashboard\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Foundation & Baseline (Weeks 1-2)\n",
    "\n",
    "**Focus:** Understand document structure without coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1: Manual Document Mapping\n",
    "\n",
    "**Objective:** Create a mental model of SOC 2 report structure\n",
    "\n",
    "**Activities:**\n",
    "1. Take 3 recent SOC 2 Type 2 reports from different auditors (Big 4 if possible)\n",
    "2. For each report, create a \"map\" document noting:\n",
    "   - Where does the auditor opinion appear? (page number, heading text)\n",
    "   - Where are report dates listed?\n",
    "   - How are TSC categories labeled?\n",
    "   - Where do exceptions appear?\n",
    "   - Where are CUECs mentioned?\n",
    "   - Where are subservice organizations listed?\n",
    "\n",
    "**Deliverable:** Google Doc with 3-column comparison table\n",
    "\n",
    "**Time:** 2-3 hours\n",
    "\n",
    "**Why This Matters:** You need to see patterns before you can code them. Different auditors format reports differently - this week you learn what's consistent vs. variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2: Current Process Baseline Measurement\n",
    "\n",
    "**Objective:** Quantify your current manual process to measure future improvement\n",
    "\n",
    "**Activities:**\n",
    "1. Time yourself doing ONE complete SOC 2 review using your current method\n",
    "2. Break down time spent:\n",
    "   - Finding each section\n",
    "   - Reading and interpreting content\n",
    "   - Copying data to your tracking system\n",
    "   - Cross-referencing with previous reviews\n",
    "3. Document pain points (e.g., \"Spent 20 minutes searching for subservice org list\")\n",
    "\n",
    "**Deliverable:** Baseline metrics document showing:\n",
    "- Total time: X minutes\n",
    "- Time breakdown by activity\n",
    "- Top 3 time wasters\n",
    "\n",
    "**Time:** 3+ hours (one full review + documentation)\n",
    "\n",
    "**Why This Matters:** You'll use these numbers to calculate ROI when presenting to your board. Concrete before/after metrics are powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2: Build Core Extraction (Weeks 3-5)\n",
    "\n",
    "**Focus:** Get Python working on actual PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3: PDF Text Extraction Setup\n",
    "\n",
    "**Objective:** Get Python to read SOC 2 PDFs and output raw text\n",
    "\n",
    "**Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required library\n",
    "# Run this in your terminal: pip install PyPDF2\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts all text from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "    \n",
    "    Returns:\n",
    "        str: All text content from the PDF\n",
    "    \"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        full_text = \"\"\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            full_text += f\"\\n--- PAGE {page_num + 1} ---\\n\"\n",
    "            full_text += page_text\n",
    "        \n",
    "        return full_text\n",
    "\n",
    "# Test it out\n",
    "test_path = \"/path/to/your/soc2_report.pdf\"  # UPDATE THIS PATH\n",
    "text = extract_text_from_pdf(test_path)\n",
    "print(f\"Extracted {len(text)} characters from PDF\")\n",
    "print(\"\\nFirst 500 characters:\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activities:**\n",
    "1. Install PyPDF2 library\n",
    "2. Run the code above on one of your SOC 2 reports\n",
    "3. Save the output to a text file and review it\n",
    "4. Note any formatting issues (tables might look weird, headers might be jumbled)\n",
    "\n",
    "**Deliverable:** Working Python script that extracts text from SOC 2 PDFs\n",
    "\n",
    "**Time:** 2-3 hours\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If text looks garbled: Try `pdfplumber` library instead\n",
    "- If you get permission errors: Make sure PDF isn't password protected\n",
    "\n",
    "**Why This Matters:** You can't analyze what you can't read. This is your foundation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4: Section Finder with Pattern Matching\n",
    "\n",
    "**Objective:** Teach Python to find specific sections using regex patterns\n",
    "\n",
    "**Concept:** Regular expressions (regex) are patterns that match text. Think of them as smart search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_opinion_section(full_text):\n",
    "    \"\"\"\n",
    "    Finds the auditor's opinion section.\n",
    "    Common patterns: \"Independent Service Auditor's Report\", \"Opinion\"\n",
    "    \"\"\"\n",
    "    # Pattern matches variations of opinion section headers\n",
    "    pattern = r\"(Independent Service Auditor[''']s Report|INDEPENDENT SERVICE AUDITOR[''']S REPORT)\"\n",
    "    \n",
    "    match = re.search(pattern, full_text, re.IGNORECASE)\n",
    "    if match:\n",
    "        start_pos = match.start()\n",
    "        # Extract 2000 characters after the match (typical opinion length)\n",
    "        opinion_text = full_text[start_pos:start_pos+2000]\n",
    "        return opinion_text\n",
    "    else:\n",
    "        return \"Opinion section not found\"\n",
    "\n",
    "def find_report_period(full_text):\n",
    "    \"\"\"\n",
    "    Finds report period dates.\n",
    "    Pattern: \"January 1, 2023 to December 31, 2023\" or similar\n",
    "    \"\"\"\n",
    "    # Matches date ranges with various formats\n",
    "    pattern = r\"(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\s+to\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\"\n",
    "    \n",
    "    match = re.search(pattern, full_text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return \"Report period not found\"\n",
    "\n",
    "# Test on your extracted text\n",
    "opinion = find_opinion_section(text)\n",
    "period = find_report_period(text)\n",
    "\n",
    "print(\"OPINION SECTION:\")\n",
    "print(opinion[:300])  # First 300 chars\n",
    "print(\"\\nREPORT PERIOD:\")\n",
    "print(period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activities:**\n",
    "1. Test these functions on 2-3 different SOC 2 reports\n",
    "2. Note when patterns DON'T match (different auditor formatting)\n",
    "3. Adjust regex patterns to handle variations\n",
    "4. Create finder functions for:\n",
    "   - TSC categories mentioned\n",
    "   - Exception sections\n",
    "   - CUEC sections\n",
    "\n",
    "**Deliverable:** Library of finder functions that work across multiple auditor formats\n",
    "\n",
    "**Time:** 3-4 hours\n",
    "\n",
    "**Regex Learning Resources:**\n",
    "- regex101.com (test patterns visually)\n",
    "- \"Match 'Report' followed by any word then a date\"\n",
    "- `r\"Report\\s+\\w+\\s+\\d{4}\"` means: \"Report\" + space + word + space + 4 digits\n",
    "\n",
    "**Why This Matters:** Generic extraction isn't useful. You need to pinpoint specific data your stakeholders care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5: Structured Data Extraction\n",
    "\n",
    "**Objective:** Extract specific data points and structure them for spreadsheet export\n",
    "\n",
    "**Full Script:** `soc2_to_spreadsheet.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_vendor_name(full_text):\n",
    "    \"\"\"\n",
    "    Extracts vendor/service organization name.\n",
    "    Usually appears early in the report.\n",
    "    \"\"\"\n",
    "    # Look for common patterns in first 2000 characters\n",
    "    header_text = full_text[:2000]\n",
    "    \n",
    "    # Pattern: \"Report on [Company Name]'s\"\n",
    "    pattern1 = r\"Report on ([A-Z][\\w\\s,.']+?)['']s\"\n",
    "    match1 = re.search(pattern1, header_text)\n",
    "    if match1:\n",
    "        return match1.group(1).strip()\n",
    "    \n",
    "    # Pattern: Company name in title case on first page\n",
    "    pattern2 = r\"([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\\s+(?:System|Controls)\"\n",
    "    match2 = re.search(pattern2, header_text)\n",
    "    if match2:\n",
    "        return match2.group(1).strip()\n",
    "    \n",
    "    return \"Unknown Vendor\"\n",
    "\n",
    "def extract_report_period(full_text):\n",
    "    \"\"\"\n",
    "    Extracts start and end dates of report period.\n",
    "    Returns dict with start_date and end_date.\n",
    "    \"\"\"\n",
    "    # Match: \"Month DD, YYYY to Month DD, YYYY\"\n",
    "    pattern = r\"(January|February|March|April|May|June|July|August|September|October|November|December)\\s+(\\d{1,2}),\\s+(\\d{4})\\s+to\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+(\\d{1,2}),\\s+(\\d{4})\"\n",
    "    \n",
    "    match = re.search(pattern, full_text[:5000])  # Check first 5000 chars\n",
    "    if match:\n",
    "        start_month, start_day, start_year = match.group(1), match.group(2), match.group(3)\n",
    "        end_month, end_day, end_year = match.group(4), match.group(5), match.group(6)\n",
    "        \n",
    "        return {\n",
    "            \"start_date\": f\"{start_month} {start_day}, {start_year}\",\n",
    "            \"end_date\": f\"{end_month} {end_day}, {end_year}\",\n",
    "            \"period_length_days\": calculate_period_length(start_month, start_day, start_year, end_month, end_day, end_year)\n",
    "        }\n",
    "    \n",
    "    return {\"start_date\": \"Not Found\", \"end_date\": \"Not Found\", \"period_length_days\": 0}\n",
    "\n",
    "def extract_opinion_type(full_text):\n",
    "    \"\"\"\n",
    "    Determines if opinion is Unqualified, Qualified, Adverse, or Disclaimer.\n",
    "    \"\"\"\n",
    "    # Find opinion section\n",
    "    opinion_pattern = r\"(Independent Service Auditor[''']s Report.*?)(Description of|Management[''']s Responsibilities)\"\n",
    "    opinion_match = re.search(opinion_pattern, full_text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    if not opinion_match:\n",
    "        return \"Opinion Not Found\"\n",
    "    \n",
    "    opinion_section = opinion_match.group(1)\n",
    "    \n",
    "    # Check for qualification language\n",
    "    if re.search(r\"qualified opinion|except for|with the exception\", opinion_section, re.IGNORECASE):\n",
    "        return \"Qualified\"\n",
    "    elif re.search(r\"adverse opinion|do not present fairly\", opinion_section, re.IGNORECASE):\n",
    "        return \"Adverse\"\n",
    "    elif re.search(r\"disclaimer|unable to obtain|scope limitation\", opinion_section, re.IGNORECASE):\n",
    "        return \"Disclaimer\"\n",
    "    elif re.search(r\"in our opinion.*fairly|presents fairly|appropriately designed|operating effectively\", opinion_section, re.IGNORECASE):\n",
    "        return \"Unqualified (Clean)\"\n",
    "    \n",
    "    return \"Unable to Determine\"\n",
    "\n",
    "def extract_tsc_categories(full_text):\n",
    "    \"\"\"\n",
    "    Identifies which Trust Service Criteria categories are covered.\n",
    "    Returns list of categories found.\n",
    "    \"\"\"\n",
    "    categories = []\n",
    "    \n",
    "    category_patterns = {\n",
    "        \"Security\": r\"\\bSecurity\\b.*?(?:criteria|principle|trust service)\",\n",
    "        \"Availability\": r\"\\bAvailability\\b.*?(?:criteria|principle|trust service)\",\n",
    "        \"Processing Integrity\": r\"\\b(?:Processing Integrity|Process Integrity)\\b\",\n",
    "        \"Confidentiality\": r\"\\bConfidentiality\\b.*?(?:criteria|principle|trust service)\",\n",
    "        \"Privacy\": r\"\\bPrivacy\\b.*?(?:criteria|principle|trust service)\"\n",
    "    }\n",
    "    \n",
    "    for category, pattern in category_patterns.items():\n",
    "        if re.search(pattern, full_text[:10000], re.IGNORECASE):\n",
    "            categories.append(category)\n",
    "    \n",
    "    return categories if categories else [\"No TSC Categories Identified\"]\n",
    "\n",
    "def extract_exception_count(full_text):\n",
    "    \"\"\"\n",
    "    Counts testing exceptions mentioned in the report.\n",
    "    Returns count and list of exception descriptions if found.\n",
    "    \"\"\"\n",
    "    # Look for exception section\n",
    "    exception_section_pattern = r\"(Tests of Controls.*?Results of Tests|Testing Exceptions?|Deviations Noted)\"\n",
    "    exception_match = re.search(exception_section_pattern, full_text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    if not exception_match:\n",
    "        # Check for \"no exceptions noted\" language\n",
    "        if re.search(r\"no exceptions? (?:were|was) noted|without exception\", full_text, re.IGNORECASE):\n",
    "            return {\"count\": 0, \"exceptions\": [\"No exceptions noted\"]}\n",
    "        return {\"count\": 0, \"exceptions\": [\"Exception section not found\"]}\n",
    "    \n",
    "    exception_text = exception_match.group(0)\n",
    "    \n",
    "    # Count bullet points or numbered exceptions\n",
    "    exception_items = re.findall(r\"(?:^|\\n)\\s*[•\\-\\*\\d+\\.)]\", exception_text)\n",
    "    count = len(exception_items)\n",
    "    \n",
    "    # Extract exception descriptions (first sentence of each)\n",
    "    exception_descriptions = re.findall(r\"[•\\-\\*\\d+\\.)\\s*([^.\\n]+)\", exception_text)[:5]  # Max 5\n",
    "    \n",
    "    return {\"count\": count, \"exceptions\": exception_descriptions}\n",
    "\n",
    "def extract_cuecs(full_text):\n",
    "    \"\"\"\n",
    "    Identifies Complementary User Entity Controls.\n",
    "    Returns count and list of CUECs if found.\n",
    "    \"\"\"\n",
    "    cuec_pattern = r\"(Complementary (?:User Entity )?Controls?|CUEC).*?(?=\\n\\n|Description of|Management)\"\n",
    "    cuec_match = re.search(cuec_pattern, full_text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    if not cuec_match:\n",
    "        return {\"count\": 0, \"cuecs\": [\"No CUECs found\"]}\n",
    "    \n",
    "    cuec_section = cuec_match.group(0)\n",
    "    \n",
    "    # Count CUEC items\n",
    "    cuec_items = re.findall(r\"(?:^|\\n)\\s*(?:CUEC)?\\s*[•\\-\\*\\d+\\.)]\", cuec_section)\n",
    "    count = len(cuec_items)\n",
    "    \n",
    "    # Extract CUEC descriptions\n",
    "    cuec_descriptions = re.findall(r\"[•\\-\\*\\d+\\.)\\s*([^.\\n]+)\", cuec_section)[:5]\n",
    "    \n",
    "    return {\"count\": count, \"cuecs\": cuec_descriptions}\n",
    "\n",
    "def extract_subservice_orgs(full_text):\n",
    "    \"\"\"\n",
    "    Identifies subservice organizations mentioned.\n",
    "    Returns list of org names.\n",
    "    \"\"\"\n",
    "    subservice_pattern = r\"(Subservice Organi[zs]ations?|Sub-?service Providers?).*?(?=\\n\\n|Description of|Management)\"\n",
    "    subservice_match = re.search(subservice_pattern, full_text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    if not subservice_match:\n",
    "        if re.search(r\"no subservice organi[zs]ations?\", full_text, re.IGNORECASE):\n",
    "            return [\"No subservice organizations\"]\n",
    "        return [\"Subservice org section not found\"]\n",
    "    \n",
    "    subservice_section = subservice_match.group(0)\n",
    "    \n",
    "    # Extract org names (typically capitalized company names)\n",
    "    org_names = re.findall(r\"\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,3})\\b\", subservice_section)\n",
    "    \n",
    "    # Remove common words that aren't company names\n",
    "    common_words = [\"Subservice\", \"Organization\", \"Provider\", \"Services\", \"Controls\", \"Management\"]\n",
    "    org_names = [org for org in org_names if org not in common_words]\n",
    "    \n",
    "    return org_names[:10] if org_names else [\"Unable to extract org names\"]\n",
    "\n",
    "def calculate_period_length(start_month, start_day, start_year, end_month, end_day, end_year):\n",
    "    \"\"\"\n",
    "    Calculates length of report period in days.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start = datetime.strptime(f\"{start_month} {start_day}, {start_year}\", \"%B %d, %Y\")\n",
    "        end = datetime.strptime(f\"{end_month} {end_day}, {end_year}\", \"%B %d, %Y\")\n",
    "        return (end - start).days\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def process_soc2_report(pdf_path):\n",
    "    \"\"\"\n",
    "    Main function that extracts all data points from a SOC 2 report.\n",
    "    \"\"\"\n",
    "    # Extract text\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        full_text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            full_text += page.extract_text()\n",
    "    \n",
    "    # Extract all data points\n",
    "    vendor_name = extract_vendor_name(full_text)\n",
    "    report_period = extract_report_period(full_text)\n",
    "    opinion_type = extract_opinion_type(full_text)\n",
    "    tsc_categories = extract_tsc_categories(full_text)\n",
    "    exception_data = extract_exception_count(full_text)\n",
    "    cuec_data = extract_cuecs(full_text)\n",
    "    subservice_orgs = extract_subservice_orgs(full_text)\n",
    "    \n",
    "    # Structure data\n",
    "    return {\n",
    "        \"Vendor Name\": vendor_name,\n",
    "        \"Report Start Date\": report_period[\"start_date\"],\n",
    "        \"Report End Date\": report_period[\"end_date\"],\n",
    "        \"Period Length (Days)\": report_period[\"period_length_days\"],\n",
    "        \"Opinion Type\": opinion_type,\n",
    "        \"TSC Categories\": \", \".join(tsc_categories),\n",
    "        \"Exception Count\": exception_data[\"count\"],\n",
    "        \"Exceptions\": \"; \".join(exception_data[\"exceptions\"][:3]),  # First 3\n",
    "        \"CUEC Count\": cuec_data[\"count\"],\n",
    "        \"CUECs\": \"; \".join(cuec_data[\"cuecs\"][:3]),  # First 3\n",
    "        \"Subservice Organizations\": \", \".join(subservice_orgs[:5])  # First 5\n",
    "    }\n",
    "\n",
    "def write_to_csv(data, output_path):\n",
    "    \"\"\"\n",
    "    Writes extracted data to CSV file.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = list(data.keys())\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        writer.writerow(data)\n",
    "    \n",
    "    print(f\"Data written to {output_path}\")\n",
    "\n",
    "# USAGE EXAMPLE\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/path/to/soc2_report.pdf\"  # UPDATE THIS\n",
    "    output_csv = \"/path/to/output.csv\"  # UPDATE THIS\n",
    "    \n",
    "    print(\"Processing SOC 2 report...\")\n",
    "    extracted_data = process_soc2_report(pdf_path)\n",
    "    \n",
    "    print(\"\\nExtracted Data:\")\n",
    "    for key, value in extracted_data.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    write_to_csv(extracted_data, output_csv)\n",
    "    print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activities:**\n",
    "1. Save the code above as `soc2_to_spreadsheet.py`\n",
    "2. Update the file paths in the usage example\n",
    "3. Run on 3 different SOC 2 reports\n",
    "4. Review the CSV output - what's accurate? What needs refinement?\n",
    "5. Adjust regex patterns based on your specific vendors' report formats\n",
    "\n",
    "**Deliverable:** Python script that outputs structured CSV data\n",
    "\n",
    "**Time:** 3-4 hours\n",
    "\n",
    "**Testing Tips:**\n",
    "- Start with one function at a time\n",
    "- Print intermediate results to see what's being matched\n",
    "- Compare script output to manual review for accuracy\n",
    "\n",
    "**Why This Matters:** Unstructured text becomes actionable data you can analyze at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 3: Scale & Integrate (Weeks 6-8)\n",
    "\n",
    "**Focus:** Process multiple reports and connect to your workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 6: Batch Processing Multiple Reports\n",
    "\n",
    "**Objective:** Process an entire folder of SOC 2 reports in one run\n",
    "\n",
    "**Full Script:** `batch_soc2_processor.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from soc2_to_spreadsheet import process_soc2_report\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename='soc2_batch_processing.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def find_pdf_files(directory):\n",
    "    \"\"\"\n",
    "    Finds all PDF files in a directory (including subdirectories).\n",
    "    \"\"\"\n",
    "    pdf_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_files.append(os.path.join(root, file))\n",
    "    return pdf_files\n",
    "\n",
    "def process_batch(input_directory, output_csv):\n",
    "    \"\"\"\n",
    "    Processes all SOC 2 reports in a directory and outputs to single CSV.\n",
    "    \n",
    "    Args:\n",
    "        input_directory (str): Path to folder containing SOC 2 PDFs\n",
    "        output_csv (str): Path for output CSV file\n",
    "    \"\"\"\n",
    "    print(f\"Scanning {input_directory} for PDF files...\")\n",
    "    pdf_files = find_pdf_files(input_directory)\n",
    "    print(f\"Found {len(pdf_files)} PDF files\\n\")\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(\"No PDF files found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    all_data = []\n",
    "    errors = []\n",
    "    \n",
    "    for i, pdf_path in enumerate(pdf_files, 1):\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        print(f\"[{i}/{len(pdf_files)}] Processing: {filename}\")\n",
    "        logging.info(f\"Processing: {pdf_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Process the report\n",
    "            data = process_soc2_report(pdf_path)\n",
    "            \n",
    "            # Add metadata\n",
    "            data[\"Source File\"] = filename\n",
    "            data[\"File Path\"] = pdf_path\n",
    "            data[\"Processing Date\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "            all_data.append(data)\n",
    "            print(f\"  ✓ Success: {data['Vendor Name']}\\n\")\n",
    "            logging.info(f\"Successfully processed: {filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing {filename}: {str(e)}\"\n",
    "            print(f\"  ✗ {error_msg}\\n\")\n",
    "            logging.error(error_msg)\n",
    "            errors.append({\"file\": filename, \"error\": str(e)})\n",
    "            \n",
    "            # Add error row to maintain record\n",
    "            error_data = {\n",
    "                \"Source File\": filename,\n",
    "                \"File Path\": pdf_path,\n",
    "                \"Processing Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"Vendor Name\": \"ERROR\",\n",
    "                \"Opinion Type\": f\"Processing failed: {str(e)}\"\n",
    "            }\n",
    "            all_data.append(error_data)\n",
    "    \n",
    "    # Write all data to CSV\n",
    "    if all_data:\n",
    "        write_batch_to_csv(all_data, output_csv)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"BATCH PROCESSING COMPLETE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total files: {len(pdf_files)}\")\n",
    "        print(f\"Successful: {len(pdf_files) - len(errors)}\")\n",
    "        print(f\"Errors: {len(errors)}\")\n",
    "        print(f\"Output saved to: {output_csv}\")\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"\\nErrors logged to: soc2_batch_processing.log\")\n",
    "            print(\"\\nFailed files:\")\n",
    "            for error in errors:\n",
    "                print(f\"  - {error['file']}: {error['error']}\")\n",
    "\n",
    "def write_batch_to_csv(data_list, output_path):\n",
    "    \"\"\"\n",
    "    Writes list of data dictionaries to CSV.\n",
    "    Handles missing fields gracefully.\n",
    "    \"\"\"\n",
    "    if not data_list:\n",
    "        return\n",
    "    \n",
    "    # Get all unique field names from all records\n",
    "    all_fields = set()\n",
    "    for data in data_list:\n",
    "        all_fields.update(data.keys())\n",
    "    \n",
    "    fieldnames = sorted(all_fields)\n",
    "    \n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for data in data_list:\n",
    "            # Fill missing fields with empty string\n",
    "            complete_data = {field: data.get(field, \"\") for field in fieldnames}\n",
    "            writer.writerow(complete_data)\n",
    "\n",
    "# USAGE EXAMPLE\n",
    "if __name__ == \"__main__\":\n",
    "    # UPDATE THESE PATHS\n",
    "    input_dir = \"/path/to/soc2_reports_folder\"  # Folder with your SOC 2 PDFs\n",
    "    output_csv = \"/path/to/batch_output.csv\"  # Where to save results\n",
    "    \n",
    "    print(\"SOC 2 Batch Processor\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Input directory: {input_dir}\")\n",
    "    print(f\"Output CSV: {output_csv}\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    process_batch(input_dir, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activities:**\n",
    "1. Create a test folder with 5-10 SOC 2 reports\n",
    "2. Run the batch processor\n",
    "3. Review the output CSV and log file\n",
    "4. Identify common errors (encoding issues, malformed PDFs, etc.)\n",
    "5. Add error handling for edge cases you discover\n",
    "\n",
    "**Deliverable:** Batch processing script that handles 50+ reports reliably\n",
    "\n",
    "**Time:** 3-4 hours\n",
    "\n",
    "**Testing Strategy:**\n",
    "- Run on small batches first (5 reports)\n",
    "- Then medium (20 reports)\n",
    "- Then your full vendor library\n",
    "\n",
    "**Common Issues & Solutions:**\n",
    "- **Encoding errors**: Use `encoding='utf-8', errors='ignore'` in file operations\n",
    "- **Memory issues**: Process files one at a time (already implemented)\n",
    "- **Timeout**: Add time.sleep(0.5) between files if needed\n",
    "\n",
    "**Why This Matters:** You can now process your entire vendor portfolio in one run. This is where 3 hours becomes 30 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 7: Google Sheets Integration\n",
    "\n",
    "**Objective:** Automatically upload processed data to Google Sheets for stakeholder access\n",
    "\n",
    "**Setup Requirements:**\n",
    "1. Enable Google Sheets API in Google Cloud Console\n",
    "2. Create service account and download credentials JSON\n",
    "3. Install libraries: `pip install gspread oauth2client`\n",
    "\n",
    "**Full Script:** `upload_to_gsheets.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_gsheets_connection(credentials_json_path):\n",
    "    \"\"\"\n",
    "    Sets up connection to Google Sheets API.\n",
    "    \n",
    "    Args:\n",
    "        credentials_json_path (str): Path to your service account credentials JSON\n",
    "    \n",
    "    Returns:\n",
    "        gspread.Client: Authorized client object\n",
    "    \"\"\"\n",
    "    scope = [\n",
    "        'https://spreadsheets.google.com/feeds',\n",
    "        'https://www.googleapis.com/auth/drive'\n",
    "    ]\n",
    "    \n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(credentials_json_path, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    \n",
    "    return client\n",
    "\n",
    "def upload_csv_to_sheet(csv_path, sheet_url, credentials_json_path, worksheet_name=\"SOC2 Data\"):\n",
    "    \"\"\"\n",
    "    Uploads CSV data to Google Sheets.\n",
    "    Creates new worksheet if doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to CSV file\n",
    "        sheet_url (str): URL of Google Sheet\n",
    "        credentials_json_path (str): Path to credentials JSON\n",
    "        worksheet_name (str): Name for the worksheet\n",
    "    \"\"\"\n",
    "    print(\"Connecting to Google Sheets...\")\n",
    "    client = setup_gsheets_connection(credentials_json_path)\n",
    "    \n",
    "    # Open the spreadsheet\n",
    "    print(f\"Opening spreadsheet...\")\n",
    "    spreadsheet = client.open_by_url(sheet_url)\n",
    "    \n",
    "    # Read CSV data\n",
    "    print(f\"Reading CSV data from {csv_path}...\")\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        data = list(csv_reader)\n",
    "    \n",
    "    print(f\"Found {len(data)} rows (including header)\")\n",
    "    \n",
    "    # Check if worksheet exists\n",
    "    try:\n",
    "        worksheet = spreadsheet.worksheet(worksheet_name)\n",
    "        print(f\"Worksheet '{worksheet_name}' found. Clearing existing data...\")\n",
    "        worksheet.clear()\n",
    "    except gspread.WorksheetNotFound:\n",
    "        print(f\"Creating new worksheet '{worksheet_name}'...\")\n",
    "        worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows=len(data)+10, cols=len(data[0])+5)\n",
    "    \n",
    "    # Upload data\n",
    "    print(\"Uploading data...\")\n",
    "    worksheet.update('A1', data)\n",
    "    \n",
    "    # Format header row\n",
    "    print(\"Formatting...\")\n",
    "    worksheet.format('A1:Z1', {\n",
    "        \"backgroundColor\": {\"red\": 0.2, \"green\": 0.4, \"blue\": 0.7},\n",
    "        \"textFormat\": {\"foregroundColor\": {\"red\": 1.0, \"green\": 1.0, \"blue\": 1.0}, \"bold\": True}\n",
    "    })\n",
    "    \n",
    "    # Freeze header row\n",
    "    worksheet.freeze(rows=1)\n",
    "    \n",
    "    print(f\"\\n✓ Upload complete!\")\n",
    "    print(f\"View at: {spreadsheet.url}\")\n",
    "    \n",
    "    return spreadsheet.url\n",
    "\n",
    "def create_dashboard_worksheet(sheet_url, credentials_json_path, data_worksheet=\"SOC2 Data\"):\n",
    "    \"\"\"\n",
    "    Creates a dashboard worksheet with summary formulas.\n",
    "    \"\"\"\n",
    "    client = setup_gsheets_connection(credentials_json_path)\n",
    "    spreadsheet = client.open_by_url(sheet_url)\n",
    "    \n",
    "    # Create dashboard worksheet\n",
    "    try:\n",
    "        dashboard = spreadsheet.worksheet(\"Dashboard\")\n",
    "        dashboard.clear()\n",
    "    except gspread.WorksheetNotFound:\n",
    "        dashboard = spreadsheet.add_worksheet(title=\"Dashboard\", rows=30, cols=10)\n",
    "    \n",
    "    # Dashboard content\n",
    "    dashboard_data = [\n",
    "        [\"SOC 2 VENDOR RISK DASHBOARD\", \"\", \"\", \"\"],\n",
    "        [f\"Last Updated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\", \"\", \"\", \"\"],\n",
    "        [\"\"],\n",
    "        [\"KEY METRICS\", \"\", \"\", \"\"],\n",
    "        [\"Total Vendors Reviewed\", f\"=COUNTA('{data_worksheet}'!A2:A)\"],\n",
    "        [\"Unqualified Opinions\", f\"=COUNTIF('{data_worksheet}'!E:E,\\\"Unqualified*\\\")\"],\n",
    "        [\"Qualified/Adverse Opinions\", f\"=COUNTIF('{data_worksheet}'!E:E,\\\"Qualified\\\")+COUNTIF('{data_worksheet}'!E:E,\\\"Adverse\\\")\"],\n",
    "        [\"Average Exception Count\", f\"=AVERAGE('{data_worksheet}'!G:G)\"],\n",
    "        [\"Vendors with 0 Exceptions\", f\"=COUNTIF('{data_worksheet}'!G:G,0)\"],\n",
    "        [\"\"],\n",
    "        [\"TSC CATEGORY COVERAGE\", \"\"],\n",
    "        [\"Security\", f\"=COUNTIF('{data_worksheet}'!F:F,\\\"*Security*\\\")\"],\n",
    "        [\"Availability\", f\"=COUNTIF('{data_worksheet}'!F:F,\\\"*Availability*\\\")\"],\n",
    "        [\"Confidentiality\", f\"=COUNTIF('{data_worksheet}'!F:F,\\\"*Confidentiality*\\\")\"],\n",
    "        [\"Processing Integrity\", f\"=COUNTIF('{data_worksheet}'!F:F,\\\"*Processing Integrity*\\\")\"],\n",
    "        [\"Privacy\", f\"=COUNTIF('{data_worksheet}'!F:F,\\\"*Privacy*\\\")\"],\n",
    "        [\"\"],\n",
    "        [\"RISK INDICATORS\", \"\"],\n",
    "        [\"Vendors with >5 Exceptions\", f\"=COUNTIF('{data_worksheet}'!G:G,\\\">5\\\")\"],\n",
    "        [\"Vendors with Subservice Orgs\", f\"=COUNTA('{data_worksheet}'!K2:K)-COUNTIF('{data_worksheet}'!K:K,\\\"*No subservice*\\\")\"],\n",
    "        [\"Reports > 1 Year Old\", f\"=COUNTIF('{data_worksheet}'!D:D,\\\">365\\\")\"],\n",
    "    ]\n",
    "    \n",
    "    dashboard.update('A1', dashboard_data)\n",
    "    \n",
    "    # Format dashboard\n",
    "    dashboard.format('A1:D1', {\n",
    "        \"backgroundColor\": {\"red\": 0.1, \"green\": 0.2, \"blue\": 0.5},\n",
    "        \"textFormat\": {\"foregroundColor\": {\"red\": 1.0, \"green\": 1.0, \"blue\": 1.0}, \"bold\": True, \"fontSize\": 14}\n",
    "    })\n",
    "    \n",
    "    dashboard.format('A4:A4', {\"textFormat\": {\"bold\": True}})\n",
    "    dashboard.format('A11:A11', {\"textFormat\": {\"bold\": True}})\n",
    "    dashboard.format('A18:A18', {\"textFormat\": {\"bold\": True}})\n",
    "    \n",
    "    print(\"✓ Dashboard created!\")\n",
    "\n",
    "# USAGE EXAMPLE\n",
    "if __name__ == \"__main__\":\n",
    "    # UPDATE THESE\n",
    "    csv_file = \"/path/to/batch_output.csv\"\n",
    "    sheet_url = \"https://docs.google.com/spreadsheets/d/YOUR_SHEET_ID/edit\"\n",
    "    credentials_json = \"/path/to/credentials.json\"\n",
    "    \n",
    "    # Upload data\n",
    "    upload_csv_to_sheet(csv_file, sheet_url, credentials_json)\n",
    "    \n",
    "    # Create dashboard\n",
    "    create_dashboard_worksheet(sheet_url, credentials_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activities:**\n",
    "1. Set up Google Cloud project and enable Sheets API\n",
    "2. Create service account and download credentials\n",
    "3. Share your target Google Sheet with the service account email\n",
    "4. Run upload script on your batch output\n",
    "5. Verify data appears correctly in Google Sheets\n",
    "6. Run dashboard creation script\n",
    "\n",
    "**Deliverable:** Automated pipeline from PDF → CSV → Google Sheets with dashboard\n",
    "\n",
    "**Time:** 3-4 hours (includes Google Cloud setup)\n",
    "\n",
    "**Setup Guide:**\n",
    "1. Go to console.cloud.google.com\n",
    "2. Create new project or select existing\n",
    "3. Enable \"Google Sheets API\" and \"Google Drive API\"\n",
    "4. Create credentials → Service account\n",
    "5. Download JSON key file\n",
    "6. Share your Google Sheet with the service account email (found in JSON)\n",
    "\n",
    "**Why This Matters:** Your leadership can now see live vendor risk data without asking you for reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 8: Dashboard Building & Visualization\n",
    "\n",
    "**Objective:** Create executive-ready visualizations in Google Sheets\n",
    "\n",
    "**Activities:**\n",
    "1. **Add Conditional Formatting:**\n",
    "   - Exception Count column: Green (0), Yellow (1-3), Red (4+)\n",
    "   - Opinion Type: Green (Unqualified), Red (Qualified/Adverse)\n",
    "   - Period Length: Yellow if >365 days (report over 1 year old)\n",
    "\n",
    "2. **Create Charts:**\n",
    "   - Pie chart: Opinion Type distribution\n",
    "   - Bar chart: Exception Count by vendor (top 10)\n",
    "   - Column chart: TSC Category coverage\n",
    "   - Line chart: Report age distribution\n",
    "\n",
    "3. **Add Filter Views:**\n",
    "   - High Risk: Qualified opinions OR >5 exceptions\n",
    "   - Needs Renewal: Reports >365 days old\n",
    "   - Clean Reports: Unqualified opinion AND 0 exceptions\n",
    "\n",
    "4. **Create Summary Tab:**\n",
    "   - Use formulas from dashboard creation script\n",
    "   - Add trend tracking (compare to previous quarter)\n",
    "   - Include top 5 riskiest vendors table\n",
    "\n",
    "**Deliverable:** Board-ready Google Sheets dashboard\n",
    "\n",
    "**Time:** 2-3 hours\n",
    "\n",
    "**Formulas You'll Use:**\n",
    "```\n",
    "=COUNTIF(range, criteria)  # Count items matching condition\n",
    "=AVERAGE(range)  # Calculate average\n",
    "=SORT(range, column, TRUE/FALSE)  # Sort data\n",
    "=FILTER(range, condition)  # Filter data\n",
    "```\n",
    "\n",
    "**Presentation Tips:**\n",
    "- Use your organization's color scheme\n",
    "- Keep it simple - 3-5 key metrics on first page\n",
    "- Include \"Last Updated\" timestamp\n",
    "- Add brief text explaining what each metric means\n",
    "\n",
    "**Why This Matters:** This is what you show the board. You're translating technical data into business value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 4: Advanced Analysis (Weeks 9-10)\n",
    "\n",
    "**Focus:** Deeper insights and risk scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 9: Exception Pattern Analysis\n",
    "\n",
    "**Objective:** Categorize and analyze types of exceptions to identify common control weaknesses\n",
    "\n",
    "**Full Script:** `exception_analyzer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Exception category keywords\n",
    "EXCEPTION_CATEGORIES = {\n",
    "    \"Access Control\": [\"access\", \"authentication\", \"authorization\", \"password\", \"user\", \"login\", \"privilege\"],\n",
    "    \"Change Management\": [\"change\", \"deployment\", \"release\", \"migration\", \"update\", \"patch\"],\n",
    "    \"Monitoring\": [\"log\", \"monitoring\", \"alert\", \"detection\", \"review\", \"audit trail\"],\n",
    "    \"Backup & Recovery\": [\"backup\", \"recovery\", \"restore\", \"disaster\", \"continuity\"],\n",
    "    \"Segregation of Duties\": [\"segregation\", \"separation\", \"duties\", \"SOD\", \"conflict\"],\n",
    "    \"Documentation\": [\"documentation\", \"policy\", \"procedure\", \"evidence\", \"record\"],\n",
    "    \"Testing\": [\"testing\", \"test\", \"validation\", \"verification\"],\n",
    "    \"Vendor Management\": [\"vendor\", \"third party\", \"subservice\", \"supplier\", \"outsourced\"]\n",
    "}\n",
    "\n",
    "def categorize_exception(exception_text):\n",
    "    \"\"\"\n",
    "    Categorizes an exception based on keyword matching.\n",
    "    Returns list of matching categories.\n",
    "    \"\"\"\n",
    "    exception_lower = exception_text.lower()\n",
    "    categories = []\n",
    "    \n",
    "    for category, keywords in EXCEPTION_CATEGORIES.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in exception_lower:\n",
    "                categories.append(category)\n",
    "                break  # Don't double-count same category\n",
    "    \n",
    "    return categories if categories else [\"Other/Uncategorized\"]\n",
    "\n",
    "def analyze_exceptions_from_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Analyzes exception patterns from batch processing CSV.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing exception patterns...\\n\")\n",
    "    \n",
    "    # Read data\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        data = list(reader)\n",
    "    \n",
    "    all_categories = []\n",
    "    vendor_categories = {}  # Track categories per vendor\n",
    "    \n",
    "    for row in data:\n",
    "        vendor = row.get('Vendor Name', 'Unknown')\n",
    "        exceptions_text = row.get('Exceptions', '')\n",
    "        \n",
    "        if exceptions_text and exceptions_text != \"No exceptions noted\":\n",
    "            # Split multiple exceptions (separated by semicolons)\n",
    "            individual_exceptions = exceptions_text.split(';')\n",
    "            \n",
    "            vendor_categories[vendor] = []\n",
    "            \n",
    "            for exception in individual_exceptions:\n",
    "                exception = exception.strip()\n",
    "                if exception:\n",
    "                    categories = categorize_exception(exception)\n",
    "                    all_categories.extend(categories)\n",
    "                    vendor_categories[vendor].extend(categories)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    category_counts = Counter(all_categories)\n",
    "    total_exceptions = len(all_categories)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"EXCEPTION CATEGORY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total exceptions analyzed: {total_exceptions}\\n\")\n",
    "    \n",
    "    print(\"Category Breakdown:\")\n",
    "    print(\"-\" * 60)\n",
    "    for category, count in category_counts.most_common():\n",
    "        percentage = (count / total_exceptions) * 100\n",
    "        print(f\"{category:<30} {count:>5} ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    # Find vendors with most diverse exception types\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VENDORS WITH MOST DIVERSE EXCEPTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    vendor_diversity = {vendor: len(set(cats)) for vendor, cats in vendor_categories.items()}\n",
    "    sorted_vendors = sorted(vendor_diversity.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    for vendor, diversity_count in sorted_vendors:\n",
    "        print(f\"{vendor:<40} {diversity_count} different categories\")\n",
    "    \n",
    "    return category_counts, vendor_categories\n",
    "\n",
    "def generate_exception_report(csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Generates detailed exception analysis report.\n",
    "    \"\"\"\n",
    "    category_counts, vendor_categories = analyze_exceptions_from_csv(csv_path)\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Category\", \"Count\", \"Percentage\"])\n",
    "        \n",
    "        total = sum(category_counts.values())\n",
    "        for category, count in category_counts.most_common():\n",
    "            percentage = (count / total) * 100\n",
    "            writer.writerow([category, count, f\"{percentage:.1f}%\"])\n",
    "    \n",
    "    print(f\"\\nDetailed report saved to: {output_path}\")\n",
    "\n",
    "# USAGE\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"/path/to/batch_output.csv\"  # Your SOC 2 data CSV\n",
    "    output_csv = \"/path/to/exception_analysis.csv\"\n",
    "    \n",
    "    generate_exception_report(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activities:**\n",
    "1. Run exception analyzer on your batch output\n",
    "2. Review category distribution - which exception types are most common?\n",
    "3. Identify vendors with diverse exception patterns (higher risk)\n",
    "4. Add custom categories relevant to your organization\n",
    "5. Create executive summary of findings\n",
    "\n",
    "**Deliverable:** Exception pattern analysis report\n",
    "\n",
    "**Time:** 2-3 hours\n",
    "\n",
    "**Analysis Questions:**\n",
    "- Which exception categories appear most frequently?\n",
    "- Are certain auditors more likely to flag specific issues?\n",
    "- Which vendors have the most concerning exception profiles?\n",
    "- What does this tell you about common industry control gaps?\n",
    "\n",
    "**Why This Matters:** You move from \"this vendor has 3 exceptions\" to \"this vendor has systemic access control issues across multiple areas.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 10: Automated Risk Scoring\n",
    "\n",
    "**Objective:** Create quantitative risk scores (0-100) based on SOC 2 findings\n",
    "\n",
    "**Scoring Methodology:**\n",
    "- Base Score: 100 (perfect)\n",
    "- Deductions:\n",
    "  - Opinion Type: Unqualified (0), Qualified (-30), Adverse (-60)\n",
    "  - Exceptions: -5 per exception (cap at -40)\n",
    "  - CUECs: -2 per CUEC (cap at -20)\n",
    "  - Subservice Orgs: -5 per org (cap at -15)\n",
    "  - Report Age: -1 per month over 12 months old (cap at -10)\n",
    "\n",
    "**Full Script:** `risk_score_calculator.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def calculate_risk_score(vendor_data):\n",
    "    \"\"\"\n",
    "    Calculates risk score (0-100) based on SOC 2 findings.\n",
    "    Lower score = higher risk.\n",
    "    \n",
    "    Args:\n",
    "        vendor_data (dict): Data from SOC 2 processing\n",
    "    \n",
    "    Returns:\n",
    "        dict: Score, breakdown, risk level\n",
    "    \"\"\"\n",
    "    score = 100\n",
    "    deductions = []\n",
    "    \n",
    "    # 1. Opinion Type\n",
    "    opinion = vendor_data.get('Opinion Type', '').lower()\n",
    "    if 'qualified' in opinion:\n",
    "        score -= 30\n",
    "        deductions.append(\"Qualified opinion: -30\")\n",
    "    elif 'adverse' in opinion:\n",
    "        score -= 60\n",
    "        deductions.append(\"Adverse opinion: -60\")\n",
    "    elif 'disclaimer' in opinion:\n",
    "        score -= 50\n",
    "        deductions.append(\"Disclaimer opinion: -50\")\n",
    "    \n",
    "    # 2. Exception Count\n",
    "    try:\n",
    "        exception_count = int(vendor_data.get('Exception Count', 0))\n",
    "        exception_deduction = min(exception_count * 5, 40)  # Cap at -40\n",
    "        if exception_deduction > 0:\n",
    "            score -= exception_deduction\n",
    "            deductions.append(f\"{exception_count} exceptions: -{exception_deduction}\")\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # 3. CUEC Count\n",
    "    try:\n",
    "        cuec_count = int(vendor_data.get('CUEC Count', 0))\n",
    "        cuec_deduction = min(cuec_count * 2, 20)  # Cap at -20\n",
    "        if cuec_deduction > 0:\n",
    "            score -= cuec_deduction\n",
    "            deductions.append(f\"{cuec_count} CUECs: -{cuec_deduction}\")\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # 4. Subservice Organizations\n",
    "    subservice_text = vendor_data.get('Subservice Organizations', '')\n",
    "    if subservice_text and \"No subservice\" not in subservice_text:\n",
    "        # Count number of orgs (comma-separated)\n",
    "        subservice_count = len([s.strip() for s in subservice_text.split(',') if s.strip()])\n",
    "        subservice_deduction = min(subservice_count * 5, 15)  # Cap at -15\n",
    "        if subservice_deduction > 0:\n",
    "            score -= subservice_deduction\n",
    "            deductions.append(f\"{subservice_count} subservice orgs: -{subservice_deduction}\")\n",
    "    \n",
    "    # 5. Report Age\n",
    "    try:\n",
    "        period_length = int(vendor_data.get('Period Length (Days)', 0))\n",
    "        if period_length > 365:\n",
    "            months_over = (period_length - 365) // 30\n",
    "            age_deduction = min(months_over, 10)  # Cap at -10\n",
    "            if age_deduction > 0:\n",
    "                score -= age_deduction\n",
    "                deductions.append(f\"Report {months_over} months old: -{age_deduction}\")\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # Determine risk level\n",
    "    if score >= 80:\n",
    "        risk_level = \"Low\"\n",
    "    elif score >= 60:\n",
    "        risk_level = \"Medium\"\n",
    "    elif score >= 40:\n",
    "        risk_level = \"High\"\n",
    "    else:\n",
    "        risk_level = \"Critical\"\n",
    "    \n",
    "    return {\n",
    "        \"score\": max(score, 0),  # Don't go below 0\n",
    "        \"risk_level\": risk_level,\n",
    "        \"deductions\": deductions\n",
    "    }\n",
    "\n",
    "def add_risk_scores_to_csv(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Reads SOC 2 data CSV, calculates risk scores, writes enhanced CSV.\n",
    "    \"\"\"\n",
    "    print(\"Calculating risk scores...\\n\")\n",
    "    \n",
    "    with open(input_csv, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        data = list(reader)\n",
    "    \n",
    "    # Calculate scores\n",
    "    for row in data:\n",
    "        risk_data = calculate_risk_score(row)\n",
    "        row['Risk Score'] = risk_data['score']\n",
    "        row['Risk Level'] = risk_data['risk_level']\n",
    "        row['Score Breakdown'] = '; '.join(risk_data['deductions']) if risk_data['deductions'] else 'No deductions'\n",
    "    \n",
    "    # Sort by risk score (lowest first = highest risk)\n",
    "    data.sort(key=lambda x: float(x.get('Risk Score', 100)))\n",
    "    \n",
    "    # Write enhanced CSV\n",
    "    if data:\n",
    "        fieldnames = list(data[0].keys())\n",
    "        \n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "    \n",
    "    print(f\"✓ Risk scores added to {output_csv}\\n\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"=\"*60)\n",
    "    print(\"RISK SCORE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    risk_levels = {'Low': 0, 'Medium': 0, 'High': 0, 'Critical': 0}\n",
    "    for row in data:\n",
    "        level = row.get('Risk Level', 'Unknown')\n",
    "        if level in risk_levels:\n",
    "            risk_levels[level] += 1\n",
    "    \n",
    "    total = len(data)\n",
    "    print(f\"Total vendors: {total}\\n\")\n",
    "    for level, count in risk_levels.items():\n",
    "        percentage = (count / total * 100) if total > 0 else 0\n",
    "        print(f\"{level:<12} {count:>4} ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOP 10 HIGHEST RISK VENDORS\")\n",
    "    print(\"=\"*60)\n",
    "    for i, row in enumerate(data[:10], 1):\n",
    "        vendor = row.get('Vendor Name', 'Unknown')\n",
    "        score = row.get('Risk Score', 'N/A')\n",
    "        level = row.get('Risk Level', 'N/A')\n",
    "        print(f\"{i:>2}. {vendor:<35} Score: {score:>3} ({level})\")\n",
    "\n",
    "# USAGE\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"/path/to/batch_output.csv\"\n",
    "    output_csv = \"/path/to/batch_output_with_scores.csv\"\n",
    "    \n",
    "    add_risk_scores_to_csv(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activities:**\n",
    "1. Run risk scorer on your batch output\n",
    "2. Review score distribution - does it match your intuition?\n",
    "3. Adjust scoring weights based on your organization's risk appetite\n",
    "4. Add additional factors relevant to your context\n",
    "5. Re-upload scored data to Google Sheets\n",
    "6. Update dashboard to show risk score distribution\n",
    "\n",
    "**Deliverable:** Risk-scored vendor dataset with methodology documentation\n",
    "\n",
    "**Time:** 3-4 hours\n",
    "\n",
    "**Validation:**\n",
    "- Pick 5 vendors you know well\n",
    "- Compare their calculated scores to your manual assessment\n",
    "- Adjust weights if scores don't align with reality\n",
    "- Document your reasoning for weight choices\n",
    "\n",
    "**Why This Matters:** Stakeholders love numbers. \"Vendor X has a risk score of 45\" is more actionable than \"Vendor X has some concerns.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 5: Production & Communication (Weeks 11-12)\n",
    "\n",
    "**Focus:** Make it sustainable and presentable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 11: Production Runbook & Documentation\n",
    "\n",
    "**Objective:** Create documentation so you (or others) can run this process repeatedly\n",
    "\n",
    "**Runbook Contents:**\n",
    "\n",
    "```markdown\n",
    "# SOC 2 Document Analysis - Production Runbook\n",
    "\n",
    "## Overview\n",
    "Automated pipeline to extract key data from SOC 2 Type 2 reports and generate risk-scored vendor dashboard.\n",
    "\n",
    "**Time Required:** 30-45 minutes (vs 3+ hours manual)\n",
    "**Frequency:** Quarterly or as new reports received\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+ installed\n",
    "- Required libraries: PyPDF2, gspread, oauth2client\n",
    "- Google Sheets API credentials (credentials.json)\n",
    "- SOC 2 PDF reports stored in designated folder\n",
    "\n",
    "## Step-by-Step Process\n",
    "\n",
    "### 1. Prepare Input Files (5 min)\n",
    "- Place new SOC 2 PDFs in: /path/to/soc2_reports/\n",
    "- Naming convention: VendorName_YYYY_SOC2.pdf\n",
    "- Verify no password-protected files\n",
    "\n",
    "### 2. Run Batch Processor (15-20 min)\n",
    "```bash\n",
    "python batch_soc2_processor.py\n",
    "```\n",
    "- Monitor output for errors\n",
    "- Check log file: soc2_batch_processing.log\n",
    "- Review CSV output: batch_output.csv\n",
    "\n",
    "### 3. Run Exception Analyzer (3 min)\n",
    "```bash\n",
    "python exception_analyzer.py\n",
    "```\n",
    "- Outputs: exception_analysis.csv\n",
    "\n",
    "### 4. Calculate Risk Scores (2 min)\n",
    "```bash\n",
    "python risk_score_calculator.py\n",
    "```\n",
    "- Outputs: batch_output_with_scores.csv\n",
    "\n",
    "### 5. Upload to Google Sheets (5 min)\n",
    "```bash\n",
    "python upload_to_gsheets.py\n",
    "```\n",
    "- Verify data uploaded correctly\n",
    "- Check dashboard formulas still working\n",
    "\n",
    "### 6. Quality Review (5-10 min)\n",
    "- Spot-check 3-5 high-risk vendors\n",
    "- Verify exception categorization makes sense\n",
    "- Confirm risk scores align with expectations\n",
    "- Note any systematic errors for script refinement\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Error: \"PDF extraction failed\"**\n",
    "- Check if PDF is corrupted (try opening manually)\n",
    "- Verify file permissions\n",
    "- Try converting to text-based PDF if scanned image\n",
    "\n",
    "**Error: \"Google Sheets authentication failed\"**\n",
    "- Verify credentials.json is in correct location\n",
    "- Check service account has edit access to sheet\n",
    "- Re-download credentials if needed\n",
    "\n",
    "**Inaccurate Extraction**\n",
    "- Identify auditor/format causing issues\n",
    "- Update regex patterns in soc2_to_spreadsheet.py\n",
    "- Add auditor-specific handling if needed\n",
    "\n",
    "## Maintenance\n",
    "\n",
    "**Monthly:**\n",
    "- Review error logs\n",
    "- Update regex patterns as new auditor formats emerge\n",
    "\n",
    "**Quarterly:**\n",
    "- Validate risk scoring weights still appropriate\n",
    "- Refine exception categorization keywords\n",
    "\n",
    "**Annually:**\n",
    "- Review SOC 2 reporting standards for changes\n",
    "- Update extraction logic if TSC criteria change\n",
    "\n",
    "## Contact\n",
    "Questions or issues: [Your Name] ([your email])\n",
    "```\n",
    "\n",
    "**Activities:**\n",
    "1. Create runbook document\n",
    "2. Test by following runbook yourself start-to-finish\n",
    "3. Have a colleague attempt to run it using only the runbook\n",
    "4. Incorporate their feedback\n",
    "5. Add to your team's knowledge base\n",
    "\n",
    "**Deliverable:** Production-ready runbook\n",
    "\n",
    "**Time:** 2-3 hours\n",
    "\n",
    "**Why This Matters:** Six months from now when you've forgotten the details, this runbook will save you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 12: Executive Presentation\n",
    "\n",
    "**Objective:** Create stakeholder-facing presentation demonstrating value\n",
    "\n",
    "**Presentation Outline:**\n",
    "\n",
    "**Slide 1: Title**\n",
    "- \"Automated SOC 2 Risk Analysis\"\n",
    "- \"Reducing Vendor Review Time by 75%\"\n",
    "\n",
    "**Slide 2: The Problem**\n",
    "- Manual SOC 2 reviews took 3+ hours per vendor\n",
    "- 500+ vendors = 1,500+ hours annually\n",
    "- Inconsistent analysis across reviewers\n",
    "- No trend tracking or portfolio-level insights\n",
    "\n",
    "**Slide 3: The Solution**\n",
    "- Automated document parsing and data extraction\n",
    "- Standardized risk scoring methodology\n",
    "- Real-time dashboard for leadership visibility\n",
    "\n",
    "**Slide 4: Results**\n",
    "- Review time: 3 hours → 30-45 minutes (75% reduction)\n",
    "- Annual time savings: 1,125 hours\n",
    "- 100% consistency in data extraction\n",
    "- Portfolio risk visibility in real-time\n",
    "\n",
    "**Slide 5: Key Insights Unlocked**\n",
    "- Top exception categories across vendor portfolio\n",
    "- Risk score distribution and trends\n",
    "- Identification of highest-risk vendors\n",
    "- Systematic analysis not possible with manual review\n",
    "\n",
    "**Slide 6: Dashboard Demo**\n",
    "- Screenshot of Google Sheets dashboard\n",
    "- Highlight key metrics and visualizations\n",
    "\n",
    "**Slide 7: ROI**\n",
    "- Time saved: 1,125 hours annually\n",
    "- Cost savings: [Your hourly rate × 1,125 hours]\n",
    "- Quality improvement: Consistent, data-driven decisions\n",
    "- Scalability: Can handle growing vendor portfolio\n",
    "\n",
    "**Slide 8: Next Steps**\n",
    "- Expand to other document types (ISO 27001, PCI DSS)\n",
    "- Integrate with GRC platform\n",
    "- Build predictive risk modeling\n",
    "\n",
    "**Activities:**\n",
    "1. Create presentation deck\n",
    "2. Prepare live demo of dashboard\n",
    "3. Calculate specific ROI numbers for your organization\n",
    "4. Practice 10-minute version and 30-minute version\n",
    "5. Anticipate questions and prepare answers\n",
    "\n",
    "**Deliverable:** Board-ready presentation with demo\n",
    "\n",
    "**Time:** 3-4 hours\n",
    "\n",
    "**Presentation Tips:**\n",
    "- Lead with business value, not technical details\n",
    "- Use before/after comparisons\n",
    "- Show actual vendor data (anonymize if needed)\n",
    "- Be ready to explain \"what could go wrong\"\n",
    "\n",
    "**Why This Matters:** This presentation gets you promoted. You've demonstrated technical capability, business value, and strategic thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab Complete! 🎉\n",
    "\n",
    "### What You've Built:\n",
    "✅ Complete SOC 2 document parsing pipeline\n",
    "✅ Automated batch processing for 500+ vendors\n",
    "✅ Exception pattern analysis framework\n",
    "✅ Risk scoring methodology\n",
    "✅ Executive dashboard with real-time data\n",
    "✅ Production documentation and runbook\n",
    "✅ Board-ready presentation\n",
    "\n",
    "### Skills Developed:\n",
    "- Python PDF processing\n",
    "- Regular expressions for text extraction\n",
    "- CSV data manipulation\n",
    "- Google Sheets API integration\n",
    "- Batch file processing\n",
    "- Risk scoring methodology design\n",
    "- Technical documentation\n",
    "- Executive communication\n",
    "\n",
    "### Measurable Impact:\n",
    "- **Time Savings:** 75% reduction (3 hours → 30-45 minutes)\n",
    "- **Consistency:** 100% standardized extraction\n",
    "- **Scalability:** 500+ vendors processed in single run\n",
    "- **Visibility:** Real-time portfolio risk dashboard\n",
    "- **Analysis Depth:** Pattern recognition impossible with manual review\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Short Term (Next Month):**\n",
    "- Run on your full vendor portfolio\n",
    "- Present to leadership\n",
    "- Gather feedback and refine\n",
    "\n",
    "**Medium Term (Next Quarter):**\n",
    "- Expand to ISO 27001, PCI DSS documents\n",
    "- Integrate with your GRC platform\n",
    "- Train team members on the system\n",
    "\n",
    "**Long Term (Next Year):**\n",
    "- Build predictive risk modeling\n",
    "- Automate remediation workflows\n",
    "- Publish your methodology (conference talk, blog post)\n",
    "\n",
    "### Questions or Issues?\n",
    "Return to any week and refine as needed. This is your system - adapt it to your evolving needs.\n",
    "\n",
    "**Congratulations on completing the lab! 🚀**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
